{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f5a61dd-2704-442a-a8f8-f7d51e2bae53",
      "metadata": {
        "id": "6f5a61dd-2704-442a-a8f8-f7d51e2bae53"
      },
      "source": [
        "# Finding the X boson (or not)\n",
        "\n",
        "10th BCD ISHEP Cargèse School, March 2025\n",
        "\n",
        "Lecturer: Bruna Pascual (bruna.pascual@cern.ch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7275614-e428-4054-b828-b12c3c94bd6d",
      "metadata": {
        "id": "d7275614-e428-4054-b828-b12c3c94bd6d"
      },
      "source": [
        "**20 years from now...** The year is 2043, and you are an experimental particle physicist working on ATLAS experiment at the high-luminosity LHC. A brilliant theory colleague of yours called Prof. Ragnie X has just written an incredible paper: she found a theory which can explain dark matter AND resolves the hierarchy problem... just by adding one new boson! (Don’t ask me how she did this, she is the brilliant one, not me!). This “X boson” should have a mass between about 500 and 1000 GeV (her theory cannot predict this accurately), and luckily for you, if it exists it should be observable at the HL-LHC via its decay to photons... so what are you waiting for? Go see if it is there and if her theory is correct!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8641905f-80a2-414d-abc0-4aca5f4ee298",
      "metadata": {
        "id": "8641905f-80a2-414d-abc0-4aca5f4ee298"
      },
      "source": [
        "**Course description:** This is the premise of your coding project, for which we have around 3hrs. You will work in python with simulated datasets to design an analysis which would be sensitive to the existence of this claimed “X boson”. Later this afternoon, you will apply this analysis to a new dataset of “HL-LHC” data, and your objective will be to determine if the “X boson” exists (with which confidence, and what mass and cross-section)... or not (in which case you will set an upper limit)! In this last part, each group will be assumed to live in parallel universes: for each of you, the existence or not of this boson, and its mass and cross-section, will be different and randomly-chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba139b3-672a-495d-9c44-349b74237e37",
      "metadata": {
        "id": "0ba139b3-672a-495d-9c44-349b74237e37"
      },
      "source": [
        "**Course outcomes:** By the end of this challenge you should have developed the following skills:\n",
        "- Know how to manipulate large datasets with tools such as `pandas Dataframes`;\n",
        "- Know how to visualise and compare distributions for different populations,\n",
        "eg with `matplotlib`;\n",
        "- Know how to use a `jupyter notebook` to analyse data;\n",
        "- Know how to apply selections to datasets in order to maxmise the signal-to-background ratio;\n",
        "- Know how to parametrise a distribution using a functional form, and so extract a backgroubd estimate;\n",
        "- Know how to test a hypothesis using the CLs method and measure observables relating to a signal;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8d93e2-0671-4857-883a-5594ac382e38",
      "metadata": {
        "id": "0f8d93e2-0671-4857-883a-5594ac382e38"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76453303-7a65-4875-af20-fca8d5e35ba0",
      "metadata": {
        "id": "76453303-7a65-4875-af20-fca8d5e35ba0"
      },
      "source": [
        "Before starting, we need to make sure you have all the datasets available. Let's get them from the git repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9872ae-53b4-4e80-9ff6-ec425c76538a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c9872ae-53b4-4e80-9ff6-ec425c76538a",
        "outputId": "72bd8ec2-c91a-4963-d6f1-973de12631b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SearchingTheXBoson'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 29 (delta 0), reused 1 (delta 0), pack-reused 27 (from 1)\u001b[K\n",
            "Receiving objects: 100% (29/29), 162.85 MiB | 20.52 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (17/17), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bpascualdias/SearchingTheXBoson.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls SearchingTheXBoson/datasets_simulation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6C2k2bD3BIm",
        "outputId": "54a6b8f5-2464-4203-a1b2-b749c0e96934"
      },
      "id": "V6C2k2bD3BIm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "background.csv\tsignal_1000.csv  signal_500.csv  signal_750.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd481898-05b9-48c8-b0e9-a12dd73bfb1c",
      "metadata": {
        "id": "cd481898-05b9-48c8-b0e9-a12dd73bfb1c"
      },
      "source": [
        "Among the downloaded content, you should be able to see a folder called `datasets_simulation` with csv files inside."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2fb13fc-d6b4-4c43-94e5-f49965d009e9",
      "metadata": {
        "id": "f2fb13fc-d6b4-4c43-94e5-f49965d009e9"
      },
      "source": [
        "# Part 1: Data wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f380f9-9bed-473c-bdce-6f643091eb2d",
      "metadata": {
        "id": "08f380f9-9bed-473c-bdce-6f643091eb2d"
      },
      "source": [
        "In the data science jargon, *data wrangling* refers to the process of tidying up and getting to grips with the data, such that one can start to analyse it. The purpose of this first session is therefore to discover the format of your datasets, to learn how to read them and make simple plots with matplotlib (or similar).\n",
        "\n",
        "The theory proposed by Dr. X. suggests that you should look for the *X boson decaying to pairs of photons*. So, you will need collision events with two photons.\n",
        "\n",
        "To search for the X boson, you have at your disposal three types of dataset:\n",
        "- simulated signal samples for a few different assumptions of the unknown X-boson mass, where the X decayed to two photons;\n",
        "- simulated background sample, which represents the processes from the standard model which also give two-photon final states;\n",
        "- a observed dataset, which contains events with two photons, but for which you do not know which (if any!) originated from an X-boson;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e3191a-9d7a-4060-b63a-3d70cfeb435b",
      "metadata": {
        "id": "59e3191a-9d7a-4060-b63a-3d70cfeb435b"
      },
      "source": [
        "**Some details in our imaginary scenario... (you will need these numbers later)**\n",
        "\n",
        "- Let's pretend that the X-boson signal samples were generated with some future version of theMadGraph Monte Carlo event generator, with hadronization and showering provided by Pythia.\n",
        "\n",
        "- Regardless of assumption of the mass of the X-boson, the **signal cross-section for this process is 5.4 picobarn for 14 TeV p-p collisions**. We can assume that **the filter efficiencyin the signal samples was 0.17** for selecting diphoton events. Each signal sample contains **10,000 such simulated events**.\n",
        "\n",
        "- Let's pretend that thediphoton background was instead generated using the Sherpa event generator, which takes care of both matrix-element and hadronization. The ** background cross-section for this process at 14 TeV was **12865 picobarn** with a **filter efficiency of 0.081**. The background sample contains **100,000 such simulated events**.\n",
        "\n",
        "- we can pretend that both types simulation are overlaid with addition simulated crossing events to mimic the presence of additional interactions in the same collision event (known as pileup). Similarly, let's say that the interaction of the simulated particles with the ATLAS detector are modelled with the GEANT 9 programme.\n",
        "\n",
        "- The data were collected within ATLAS during pp collision events at 14 TeV, during the years 2031-2043, during which time an integrated luminosity of **1298.1 /fb of data was accumulated**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a457320f-6fd8-42c7-9d79-49abb65dd99e",
      "metadata": {
        "id": "a457320f-6fd8-42c7-9d79-49abb65dd99e"
      },
      "source": [
        "In order to avoid biasing yourself and designing your analysis around possible statistical fluctuations, you will be performing a “blind analysis”. This means that the observed data (which will be in the exact same format as the other two types of dataset), will not be made available to you until the very end of the day. You should make all your analysis decisions before you actually look at the data! The history of particle physics is littered with false discoveries by people who took shortcuts by peeking at the data, or changing their analysis after looking at the data. We will not fall into that trap. But let’s start to get familiar with the samples you have at your disposal now...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc427b0-d163-4873-9c6c-4f51ea0a0572",
      "metadata": {
        "id": "2dc427b0-d163-4873-9c6c-4f51ea0a0572"
      },
      "source": [
        "## First contact with the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e046b79-1024-472d-be56-1bf996c8cc2a",
      "metadata": {
        "id": "5e046b79-1024-472d-be56-1bf996c8cc2a"
      },
      "source": [
        "On the indico page for the course, you will find several files called `signal_xxx.csv` which contain each 10,000 simulated events for different assumptions of the X-boson mass. Each line (aside from the headers) corresponds to one event. Each columns represents one property. There is also a `background.csv` file in the same format with 100,000 simulated backgroudn events."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca178ef9-d366-4356-a966-306eea654073",
      "metadata": {
        "id": "ca178ef9-d366-4356-a966-306eea654073"
      },
      "source": [
        "**TASK 1.1** First of all, let's load the data from the csv files into tables that we can easily manipulate.\n",
        "\n",
        "Start by doing an `import` of the pandas library via `import pandas as pd` and read in the different datasets. What sort of information do you have in the dataset? Inspect it using the `Dataframe.head()` method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# ..."
      ],
      "metadata": {
        "id": "G7yQzGSZy1KQ"
      },
      "id": "G7yQzGSZy1KQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b665d88f-ec26-44d8-959c-0606b4a5751a",
      "metadata": {
        "id": "b665d88f-ec26-44d8-959c-0606b4a5751a"
      },
      "source": [
        "**TASK 1.2** Next, make sure you know how to manipulate `pandas Dataframe`. Can you:\n",
        "- select the information for one column (for example, just the \"photon1_pT\")?\n",
        "- select the information for one row (form example, the 111th row)?\n",
        "- apply a mask/selection based on some of the variables (for example, pick only the dataframe rows where photon1_pT > 250 GeV)? How many entries are in the dataframe after such a cut?\n",
        "\n",
        "You can use google to find examples if you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950d1923-2796-481b-83e1-2c1e1870acd9",
      "metadata": {
        "id": "950d1923-2796-481b-83e1-2c1e1870acd9"
      },
      "outputs": [],
      "source": [
        "# one column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2f2457-83dd-4d58-a887-832875eebc5d",
      "metadata": {
        "id": "7b2f2457-83dd-4d58-a887-832875eebc5d"
      },
      "outputs": [],
      "source": [
        "# one row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2380e2d5-c14a-4b58-8ddf-bc43d18db7e5",
      "metadata": {
        "id": "2380e2d5-c14a-4b58-8ddf-bc43d18db7e5"
      },
      "outputs": [],
      "source": [
        "# select just rows where photon1_pT > 150 GeV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3591d2-60b9-42d4-97fc-84aeeba53892",
      "metadata": {
        "id": "8c3591d2-60b9-42d4-97fc-84aeeba53892"
      },
      "source": [
        "**TASK 1.3:** Now that we can read in a file and access the observables from each event, we want to start making histograms, showing the distribution of each observable for each sample type. What we want is to draw several sample histograms on the same plot.\n",
        "\n",
        "It may be worth defining a function to do this plotting step for you, taking as input the name of an observable and plotting histograms for the different datasets. A skeleton is already provided, but the plotting itself is missing...\n",
        "\n",
        "- Add the lines that would make a histogram of the observable, making sure that it’s normalised to unit area.\n",
        "- You can use `matplotlib.pyplot`’s `plt.hist()` function for this, with the `density=1` keyword for the unit normalisation. You'll need to import it `import matplotlib.pyplot as plt`\n",
        "- To make sure the plots use the same binning for all the datasets, we can use the fact that `plt.hist()` returns the binning. You can catch it like this and use it for the next plot:\n",
        "```\n",
        "values, bins, other_stuff = plt.hist(data1, density=1, label=\"blah1\")\n",
        "plt.hist(data2, bins=bins density=1, label=\"blah2\")\n",
        "```\n",
        "- Note that you can also use the `alpha=0.5` argument to make the plots slightly transparent.\n",
        "\n",
        "Using the function, start by visualising how the `photon1_pT` compares between the different datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092c53b0-6b85-42f4-b39e-25cd8a868732",
      "metadata": {
        "id": "092c53b0-6b85-42f4-b39e-25cd8a868732"
      },
      "outputs": [],
      "source": [
        "# function definition\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_distribution(observable):\n",
        "\n",
        "     _, bins, _ = plt.hist(df_bkg[observable], alpha=0.4, histtype='barstacked', stacked=False, density=1, label=\"Background\")\n",
        "     plt.hist(df_sig500[observable],bins=bins, alpha=0.4, histtype='barstacked', stacked=False, density=1, label=\"$m_X=500$ GeV\")\n",
        "     plt.hist(df_sig750[observable], alpha=0.4, bins=bins, histtype='barstacked', stacked=False, density=1, label=\"$m_X=750$ GeV\")\n",
        "     plt.hist(df_sig1000[observable], alpha=0.4,bins=bins, histtype='barstacked', stacked=False, density=1, label=\"$m_X=1000$ GeV\")\n",
        "     plt.legend()\n",
        "     plt.xlabel(observable)\n",
        "     plt.ylabel(\"a.u.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4eb28e5-7530-4c96-826a-d280517bcc34",
      "metadata": {
        "id": "d4eb28e5-7530-4c96-826a-d280517bcc34"
      },
      "outputs": [],
      "source": [
        "# call the function to see the distribution of the observable photon1_pT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40f1858-114d-40da-932a-6e4f52ea419f",
      "metadata": {
        "id": "f40f1858-114d-40da-932a-6e4f52ea419f"
      },
      "source": [
        "Now let's use a loop to make plots for all variables. For this, make sure to add `%matplotlib inline` to the cell and `plt.show()` inside the loop and after your function.\n",
        "\n",
        "Hint: You can obtain the column names to loop over with `df.columns`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42a1f71-45df-4ed5-a7a3-cdd8ba86b5bb",
      "metadata": {
        "id": "b42a1f71-45df-4ed5-a7a3-cdd8ba86b5bb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# hint: do something like this\n",
        "# for colName in df.columns:\n",
        "#    plotDistribution(colName)\n",
        "#    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9068b37-7577-4e9d-a23b-e117edd3ece0",
      "metadata": {
        "id": "f9068b37-7577-4e9d-a23b-e117edd3ece0"
      },
      "source": [
        "Before we move to the next section, it's worth inspecting these various variables and seeing if we can already spot those where the distribution from signal and background is different. Indeed, some variables look the same for signal and background, others show some differences. This means we can apply *selections* (aka *cuts*) where we can remove background while retaining a high propotion of signal... and thus improve our sensitivity to the prospective X boson!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5b3082-154f-4b78-9b82-b146d63e7926",
      "metadata": {
        "id": "1e5b3082-154f-4b78-9b82-b146d63e7926"
      },
      "source": [
        "# Part 2: Invariant mass ...and normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aacfe725-c146-4270-9ea2-04d6df2dacc5",
      "metadata": {
        "id": "aacfe725-c146-4270-9ea2-04d6df2dacc5"
      },
      "source": [
        "If the X decays to pairs of photons, then we expect a resonance in the di-photon invariant mass spectrum. In other words, the invariant mass of the two-photon system should be the same as the invariant mass of the X-boson... which ought to be distributed approximately like a Gaussian whose mean is the mass of the boson. The invariant mass spectrum of the background should not be distributed in such a way, but is more likely to be some kind of falling spectrum. So, let’s calculate the invariant mass and plot that too. this will become our discriminating variable. The formula relating invariant mass of a two-body system to its transverse momenta ($p_T$), pseudorapidity (η) and azimuthal angle (φ) is:\n",
        "\n",
        "$$m_{inv} = \\sqrt{2 p^T_1 p^T_2 (\\cosh(η_1 − η_2) − \\cos(φ_1 − φ_2))} . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79fb30ef-3fbb-4b43-ac45-83feb31a3456",
      "metadata": {
        "id": "79fb30ef-3fbb-4b43-ac45-83feb31a3456"
      },
      "source": [
        "**TASK 2.1:** Using the provided formula, add a column into each dataframe for the invariant mass of the two-photon system (using vectorized operations on the df columns!). Plot the distribution of that for signal and background as you did for the other variables\n",
        "\n",
        "Hint: `numpy` has functions for cosh and cos. You may want to make a function to avoid repeating yourself. You can import it via `import numpy as np`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b606ab46-fac5-43d8-97fa-f8c472c91896",
      "metadata": {
        "id": "b606ab46-fac5-43d8-97fa-f8c472c91896"
      },
      "outputs": [],
      "source": [
        "def add_minv(df):\n",
        "    # Fill in here!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99cb0aa-0acc-45f0-acb2-a42ad57cda35",
      "metadata": {
        "id": "c99cb0aa-0acc-45f0-acb2-a42ad57cda35"
      },
      "outputs": [],
      "source": [
        "# simething like this?\n",
        "#df_sig500 = add_minv(df_sig500)\n",
        "#...\n",
        "#df_bkg = add_minv(df_bkg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2f2872-6b3f-4b40-9bf2-888cc19979fe",
      "metadata": {
        "id": "ae2f2872-6b3f-4b40-9bf2-888cc19979fe"
      },
      "outputs": [],
      "source": [
        "# plot invariant mass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9f85bb-d783-49e9-a2d9-8574ca2d322e",
      "metadata": {
        "id": "1c9f85bb-d783-49e9-a2d9-8574ca2d322e"
      },
      "source": [
        "You should have found that the background invariant mass distribution is a falling spectrum, while the signals have gaussian invariant mass distributions centred around their hypothetical mass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa4c6dc-c8b7-4416-acf6-2b7c20cc07c8",
      "metadata": {
        "id": "2fa4c6dc-c8b7-4416-acf6-2b7c20cc07c8"
      },
      "source": [
        "We now would like to plot predicted event counts instead of distributions normalised to 1.\n",
        "So what we would like is that the integral of each of the invariant mass plots should correspond to the predicted number of events at the LHC. The general formula to get the number of expected events in a sample is:\n",
        "$$N_{exp}=L×σ×ε× \\frac{N_{sel}}{ N_{gen}}$$\n",
        "\n",
        "where $N_{exp}$ is the expected number of events in collisions, $L$ is the integrated luminosity of the collected dataset, $σ$ is the cross-section for the process under consideration, $ε$ is the filter-efficiency (if we restrict the event generation for example to a particular decay type of the particle, $X$ → γγ),  $N_{sel}$ is the number of selected events in your analysis, and $N_{gen}$ is the number of events generated, before any other selections.\n",
        "In your case, to calculate individual event weights, you can take $N_{sel} = 1$, and $N_{gen}$ is the total number of events in the samples you have been given. **The required values for each dataset are given in the preamble**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8069aa6-a5c7-4c61-88cb-934ec15aa2ee",
      "metadata": {
        "id": "f8069aa6-a5c7-4c61-88cb-934ec15aa2ee"
      },
      "source": [
        "**TASK 2.2** Calculate the normalisation weight for each sample, add at it as an extra column in each dataset, named `'w'` or `'weight'`. Then make a new function called `plot_weighted_distribution` which plots the sample plots as above, but where the samples are normalised using the weight you just calculated. You can remove the `density=1` keyword and replace it with the `weights=...` keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7dab0b-730f-44a4-bc4e-42ec7d58a6a8",
      "metadata": {
        "id": "fd7dab0b-730f-44a4-bc4e-42ec7d58a6a8"
      },
      "outputs": [],
      "source": [
        "# example... df_bkg[\"w\"] = 1298.1 * 5.4 * 0.17 * 1 / len(df_bkg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9882f097-ba93-4d88-b45d-0243459bc956",
      "metadata": {
        "id": "9882f097-ba93-4d88-b45d-0243459bc956"
      },
      "outputs": [],
      "source": [
        "# hint : you can amend the code below to work with your notebook!\n",
        "def plot_weighted_distribution(observable):\n",
        "\n",
        "     _, bins, _ = plt.hist(df_bkg[observable], alpha=0.4, histtype='barstacked', stacked=False, bins=100, weights=df_bkg.w, label=\"Background\")\n",
        "     plt.hist(df_sig500[observable],bins=bins, alpha=0.4, histtype='barstacked', stacked=False,weights=df_sig500.w, label=\"$m_X=500$ GeV\")\n",
        "     plt.hist(df_sig750[observable], alpha=0.4, bins=bins, histtype='barstacked', stacked=False, weights=df_sig750.w,label=\"$m_X=750$ GeV\")\n",
        "     plt.hist(df_sig1000[observable], alpha=0.4,bins=bins, histtype='barstacked', stacked=False, weights=df_sig1000.w,label=\"$m_X=1000$ GeV\")\n",
        "     plt.legend()\n",
        "     plt.xlabel(observable)\n",
        "     plt.ylabel(\"a.u.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11fb68b-121b-46ab-b616-55e79e94de17",
      "metadata": {
        "id": "a11fb68b-121b-46ab-b616-55e79e94de17"
      },
      "outputs": [],
      "source": [
        "# plot invariant mass after normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e1b02ba-92dd-4255-bcf1-49498da0c28e",
      "metadata": {
        "id": "1e1b02ba-92dd-4255-bcf1-49498da0c28e"
      },
      "source": [
        "You should now be able to see a small signal which is well inside the background. How can we make it more visible?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e1c289-2435-4b62-bf9d-d8f966b7eb96",
      "metadata": {
        "id": "67e1c289-2435-4b62-bf9d-d8f966b7eb96"
      },
      "source": [
        "# Part 3: Designing the selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c45827-8493-4d3e-8e0b-d317c27e3e19",
      "metadata": {
        "id": "94c45827-8493-4d3e-8e0b-d317c27e3e19"
      },
      "source": [
        "So far, you have become familiar with your datasets and learnt to make some plots to show the simulated distributions of signals and backgrounds in your search for the X boson. But as we saw, the correctly-normalised signal is tiny compared to the background, and without further effort it’s unlikely that we’d be able to distinguish it from the background within the uncertainties. So, we need to try to make our analysis more sensitive! Now, in the “true” dataset, the background may not be perfectly modelled, and the signal (if it is there!) is unlikely to look exactly like the ones you are practicing with. But you can assume that these simulated samples are close enough to what you will encounter in the “observed” data to be able to design a selection which is optimised for your analysis.\n",
        "The first thing we need to do is try to get a realistic estimate of the background and signal we might expect to find. Remember that the X boson is only expected to exist in a given range of possible masses, between 500 and 1000 GeV. So we can already define a first search region (SR) where we will be looking for our signal. To have a safety margin, we can add a first selection (or in the jargon a cut ):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e0073d-f8d3-4af3-9a0d-13d869abd1b1",
      "metadata": {
        "id": "d9e0073d-f8d3-4af3-9a0d-13d869abd1b1"
      },
      "source": [
        "The first thing we need to do is try to get a realistic estimate of the background and signal we might expect to find. Remember that the X boson is only expected to exist in a given range of possible masses, between 500 and 1000 GeV. So we can already define a first search region (SR) where we will be looking for our signal. To have a safety margin, we can add a first selection (or in the jargon a \"cut\" ):\n",
        "\n",
        "$$m_{inv} ∈ [400, 1100] \\text{ GeV}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "233de3c8-c031-4db7-9431-d738767f08b2",
      "metadata": {
        "id": "233de3c8-c031-4db7-9431-d738767f08b2"
      },
      "source": [
        "**TASK 3.1:** Count the (weighted) number of background events predicted in the background simulation in your basic SR by applying the invariant mass cut and taking the sum of weights of the events which survive. Do the same for one of your signal samples. You can then calculate your sensitivity $s$ as\n",
        "$$ s = \\frac{\\sum w_{\\text{signal}}}{\\sqrt{\\sum w_\\text{bkg}}} $$\n",
        "\n",
        "How many entries survived the cut?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d663072f-9aeb-4279-be76-67136fe2cb25",
      "metadata": {
        "id": "d663072f-9aeb-4279-be76-67136fe2cb25"
      },
      "outputs": [],
      "source": [
        "# hint: first, use the new \"minv\" variable you made in task 2.1\n",
        "# to define a mask to select only masses within 400-1100 GeV.\n",
        "# Make a copy of your background dataframe and apply the mask to it.\n",
        "# Do the same for your signal sample(s).\n",
        "# extra hint: you can use np.logical_and(mask1, mask2) to make a combination of masks,\n",
        "# for example minv > 400 and minv < 1100\n",
        "\n",
        "# somthing like...\n",
        "# mask_bkg = np.logical_and(df_bkg[\"minv\"] < 1100 , df_bkg[\"minv\"] > 400)\n",
        "# df_bkg_minv = df_bkg[mask_bkg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92971d41-72ed-4676-a275-85747078d38b",
      "metadata": {
        "id": "92971d41-72ed-4676-a275-85747078d38b"
      },
      "outputs": [],
      "source": [
        "# hint: create a function that takes as input the background and signal dataframes\n",
        "# and returns the sensitivity. To get the sum of weights in your new reduced dataframe, use\n",
        "# np.sum(df['weight']), where 'weight' is the column you added in Task 2.2\n",
        "def get_sensitivity(df_s, df_b):\n",
        "    # s = ...\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6b1e80-c6b5-4913-ad8d-0a10ff8a2d4f",
      "metadata": {
        "id": "4c6b1e80-c6b5-4913-ad8d-0a10ff8a2d4f"
      },
      "outputs": [],
      "source": [
        "# calculate the sensitivity for each one of the signal datasets, you should get around 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "994446b1-d0eb-4898-a2d5-4e9b8bb7ded8",
      "metadata": {
        "id": "994446b1-d0eb-4898-a2d5-4e9b8bb7ded8"
      },
      "source": [
        "$s$ is what we call a *figure of merit*, the higher it is, the better is our analysis and the more chance we have to discover a new particle if it's there, or the stronger the limit we can set if we see no new particle. The `sqrt` is there because `sqrt(bkg)` corresponds roughly to the *uncertainty* on the background estimate. So this $s$ is effectively telling you how large your predicted signal is compared to your background."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5f4326-8835-4e3e-b1d8-40fa7969f619",
      "metadata": {
        "id": "4b5f4326-8835-4e3e-b1d8-40fa7969f619"
      },
      "source": [
        "What we want to do now is try to apply additional selections to *maximise* $s$ as much as possible. In other words, we want to get the most signal we can while removing the most background.  In general we will not really want to touch any of the variables which might contribute to the `m_inv` calculation (we want to avoid biasing this spectrum for reasons which we will see later). But you have several other variables which are a priori uncorrelated with the invariant mass, and where we could get some extra sensitivity. For example, if you look back at the distribution of the `n_jets`, you see that there are clearly regions where we can add a cut which would remove background without really removing any signal, which would obviously improve our $s$!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40df6cd1-c21a-4cfd-8a60-6b0353535a5a",
      "metadata": {
        "id": "40df6cd1-c21a-4cfd-8a60-6b0353535a5a"
      },
      "source": [
        "**TASK 3.2:** Try to define a new mask which excludes some parts of the `n_jets` spectrum, where there is more background than signal. For example `n_jets < 5` (check this on your plots!).\n",
        "Use that to make new dataframes with this new selection applied alongside your \"minv\" mass. What are your new values of $s$ after applying this selection ?\n",
        "I got $s=4.4$ or so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c9760c-28d2-4480-bffd-9638d08b7cb0",
      "metadata": {
        "id": "b9c9760c-28d2-4480-bffd-9638d08b7cb0"
      },
      "outputs": [],
      "source": [
        "# hint, you can maybe define a new mask using np.logical_and(mask_SR, df['n_jets'] < 5) and apply it to your original dataset.\n",
        "# alternatively, you can just apply the same process as you did in Task 3.1 but on your already-reduced dataset.\n",
        "#\n",
        "# it could help to define a function for this sort of thing, if you like\n",
        "def apply_selection(df, variable, cutValue = 0, upperOrLower= \">\"):\n",
        "    if upperOrLower==\"<\":\n",
        "        mask= df[variable] < cutValue\n",
        "    elif upperOrLower==\">\":\n",
        "        mask= df[variable] > cutValue\n",
        "    else:\n",
        "        print(f'{upperOrLower} not a defined operation!!')\n",
        "    return df[mask]\n",
        "\n",
        "# and call it something like this:\n",
        "# cutVal=5\n",
        "# df_bkg_SR_after_cuts = apply_selection (df_bkg_SR , \"n_jets\" , cutVal, \"<\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get sensitivity after applying the selection on the number of jets\n"
      ],
      "metadata": {
        "id": "s8cVc2yCVx3s"
      },
      "id": "s8cVc2yCVx3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e830251f-6310-4c42-8016-6d97fd5ec2a1",
      "metadata": {
        "id": "e830251f-6310-4c42-8016-6d97fd5ec2a1"
      },
      "source": [
        "Try different values of the `n_jets` cut (like 1, 2, 3, 4, 5, 6, 7, 8, 9...) to see which of them gives you the best values of $s$.\n",
        "The best value I got was $s=5.6$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24538a94-4e3f-46f7-a144-179127447556",
      "metadata": {
        "id": "24538a94-4e3f-46f7-a144-179127447556"
      },
      "outputs": [],
      "source": [
        "# loop over integer values between 1 and 10 and print the sensivity for each njet choice\n",
        "# hint: here is how I did it, can you do something similar?\n",
        "# for nj in range(1,9):\n",
        "#     df_bkg_SR_after_cuts = apply_selection (df_bkg_SR , \"n_jets\" , nj, \"<\")\n",
        "#     df_sig500_SR_after_cuts = apply_selection (df_sig500_SR , \"n_jets\" , nj, \"<\")\n",
        "#     s = get_sensitivity(df_sig500_SR_after_cuts, df_bkg_SR_after_cuts)\n",
        "#     print(f\"For njets > {nj}, we get sensitivity {s:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa240cc-f135-4fe8-b90d-cb20b0fc3272",
      "metadata": {
        "id": "4fa240cc-f135-4fe8-b90d-cb20b0fc3272"
      },
      "source": [
        "**TASK 3.3:** Now that you know how to apply a cut, you can look at the other variables and apply additional selections for those, combining the selections on all the variables to get your final selection.\n",
        "\n",
        "The variables you should consider are\n",
        "- n_jets\n",
        "- n_leptons\n",
        "- photon1_isolation\n",
        "- photon2_isolation\n",
        "  \n",
        "(but of course adding in the others may add extra sensitivity)\n",
        "\n",
        "The simplest thing to do is to look at the plots you made above and estimate what selections to make by eye, calculate the new $s$, and manually play around with the selections to see when you reach a maximum.\n",
        "\n",
        "If you are feeling more ambitious you could write a function which automatically scans the range of possible cuts to get the one with higest $s$ for each variables.\n",
        "\n",
        "If you are feeling REALLY ambitious you could train a simple machine learning algorithm to simulatenously epxloit all these variables in the most optimal way :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30922f3c-dbd9-41a9-a1b3-56ab4c553a81",
      "metadata": {
        "id": "30922f3c-dbd9-41a9-a1b3-56ab4c553a81"
      },
      "outputs": [],
      "source": [
        "# hint: this is how I did it... perhaps you can do something similar?\n",
        "\n",
        "# njetCut = ???\n",
        "# nLeptonsCut = ???\n",
        "# photon1_isolationCut = ???\n",
        "# photon2_isolationCut = ???\n",
        "\n",
        "# df_bkg_selection = apply_selection (df_bkg_SR , \"n_jets\" , njetCut, \"<\")\n",
        "# df_bkg_selection = apply_selection (df_bkg_selection , \"n_leptons\" , nLeptonsCut, \">\")\n",
        "# df_bkg_selection = apply_selection (df_bkg_selection , \"photon1_isolation\" , photon1_isolationCut, \"<\")\n",
        "# df_bkg_selection = apply_selection (df_bkg_selection , \"photon2_isolation\" , photon2_isolationCut, \"<\")\n",
        "\n",
        "# df_sig500_selection = apply_selection (df_sig500_SR , \"n_jets\" , njetCut, \"<\")\n",
        "# df_sig500_selection = apply_selection (df_sig500_selection , \"n_leptons\" , nLeptonsCut, \">\")\n",
        "# df_sig500_selection = apply_selection (df_sig500_selection , \"photon1_isolation\" , photon1_isolationCut, \"<\")\n",
        "# df_sig500_selection = apply_selection (df_sig500_selection , \"photon2_isolation\" , photon2_isolationCut, \"<\")\n",
        "\n",
        "# s = get_sensitivity(df_sig500_selection, df_bkg_selection)\n",
        "# print(f\"Sensitivity is {s:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8bcfc4-7ef0-4d78-ae7a-939ce946f151",
      "metadata": {
        "id": "3f8bcfc4-7ef0-4d78-ae7a-939ce946f151"
      },
      "source": [
        "I was able to get a $s$ around 7 using the four basic variables listed above and optimising \"by hand\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217bcdc3-04a8-45ca-861d-3d99a62e9814",
      "metadata": {
        "id": "217bcdc3-04a8-45ca-861d-3d99a62e9814"
      },
      "source": [
        "**TASK 3.4** Now that you have defined your set of selections, apply this to your original dataframe (without the $m_{inv}$ window cut) and re-make your plot of the invariant mass spectrum.\n",
        "Does this look promising in terms of a potential discovery ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25012b25-2e5f-4b1c-9a40-7479f8ae25f1",
      "metadata": {
        "id": "25012b25-2e5f-4b1c-9a40-7479f8ae25f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6a3c8125-dc18-415e-8718-faf677f64f42",
      "metadata": {
        "id": "6a3c8125-dc18-415e-8718-faf677f64f42"
      },
      "source": [
        "# Part 4: Statistical analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369ef232-4276-41aa-99cc-c77860360db9",
      "metadata": {
        "id": "369ef232-4276-41aa-99cc-c77860360db9"
      },
      "source": [
        "Now that you have defined your selection, you need to perform the statistical analysis: estimating the amount of background in your signal region, and determining what you will do if you see an excess of events above your estimate: measuring the significance and the mass of the X boson if you have an excess... and setting limits on the cross-section if not!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd78f40e-2ce6-4212-9407-44c6d8d6060c",
      "metadata": {
        "id": "dd78f40e-2ce6-4212-9407-44c6d8d6060c"
      },
      "source": [
        "The first step will be the background estimate: your background simulation is not guaranteed to be a perfect description of your data. If was fine to derive your selection, but if there are small differences in the simulation and what nature serves you in the distribution of the background invariant mass spectrum, then you might over- or under-estimate your prediction in the SR, leading potentially to a false discovery claim or a missed discovery!\n",
        "How do we deal with this possibility? The best way forward is to use a so- called data-driven background estimate. In a nutshell, you can use the events which pass into “sideband” region which is similar to your SR but inverting the m_inv window, to estimate the background. Practically speaking, this means to parameterise the shape of the falling invariant mass spectrum using the events in side- band), where you know there is no signal. Then use the parametrised function to guess the background contribution in the SR (and it’s uncertainty!). Then when we look at the data in that region, we can check if there is an excess of events compared to what is predicted. Let’s test this procedure first on your background simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a930997-538a-46f2-af83-e2cd2457142d",
      "metadata": {
        "id": "9a930997-538a-46f2-af83-e2cd2457142d"
      },
      "source": [
        "**TASK 4.1:** Using your simulated background events, make a plot of the sideband of the `m_inv` spectrum of your SR: in other words, plot events passing all the cuts in your selection, but inverting the invariant mass cut:\n",
        "$$m_\\text{inv} \\notin [400, 1100] \\text{ GeV}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1a1dd9-90fd-467a-a8a6-7ca7e1a8583e",
      "metadata": {
        "id": "4a1a1dd9-90fd-467a-a8a6-7ca7e1a8583e"
      },
      "outputs": [],
      "source": [
        "# hint: you can amend the code below to work with your code\n",
        "# do you understand what each step is doing?\n",
        "\n",
        "# mask_sideband_bkg = np.logical_or(df_bkg_selection[\"minv\"]< 400, df_bkg_selection[\"minv\"]> 1100)\n",
        "# df_bkg_sideband = df_bkg_selection[mask_sideband_bkg]\n",
        "# df_bkg_sr = df_bkg_selection[~mask_sideband_bkg]\n",
        "\n",
        "# n, bins, _ = plt.hist(df_bkg_sideband.minv, bins=100, weights=df_bkg_sideband.w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6173c7-0927-4682-a10e-fded1d160462",
      "metadata": {
        "id": "6a6173c7-0927-4682-a10e-fded1d160462"
      },
      "source": [
        "It looks like this shape could be fitted by an exponential...\n",
        "Parametrising a distribution means fitting a function to the shape. A good option for this is to use the `scipy.optimize.curve_fit` package. If you import the `helpers.py` (written by me tpo simplify this tutorial) package, you fill find a convenient wrapper do fit, some predefined functions (you can add your own too!) and a helper to work out the post-fit yield in a given range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710dccc4-4b9d-42fe-85c4-6387ee86d6f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "710dccc4-4b9d-42fe-85c4-6387ee86d6f1",
        "outputId": "83c11a4b-e98c-41b6-b83e-3b4123291099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helpers.py\n"
          ]
        }
      ],
      "source": [
        "!ls SearchingTheXBoson | grep help"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are also going to need to install the `pyhf` package, a useful library to help perform the statistical analysis."
      ],
      "metadata": {
        "id": "U8JbTDlR84rH"
      },
      "id": "U8JbTDlR84rH"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhf"
      ],
      "metadata": {
        "id": "vFNYBRPj85Au"
      },
      "id": "vFNYBRPj85Au",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "19d4d62e-7128-46cc-ba8f-c0d75cd5c426",
      "metadata": {
        "id": "19d4d62e-7128-46cc-ba8f-c0d75cd5c426"
      },
      "source": [
        "**TASK 4.2:** Import the `helpers.py` package.\n",
        "Adjust the code in the hint below to fit your sidebands to an exponential function using the provided fitting tool (which is using `scipy.curve_fit` under thehood).\n",
        "\n",
        "Again, by amending the code in the hint, determine the estimated background yield in the SR: effectively, the integral of this function over the SR’s invariant mass window  [400, 1100] GeV. This is your **background estimate**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97dc3f6-f863-4ec7-8cbb-7f68282b2aa0",
      "metadata": {
        "id": "e97dc3f6-f863-4ec7-8cbb-7f68282b2aa0"
      },
      "outputs": [],
      "source": [
        "# hint: you can amend the code below to work with your code\n",
        "# do you understand what each step is doing?\n",
        "\n",
        "# import SearchingTheXBoson.helpers as hp\n",
        "# n, bins, _ = plt.hist(df_bkg_sideband.minv, bins=np.linspace(50,2000, 40), weights=df_bkg_sideband.w)\n",
        "# plt.hist(df_bkg_sr.minv, bins=np.linspace(50,2000, 40), weights=df_bkg_sr.w)\n",
        "# x, params, errs = hp.do_fit(hp.Expo, bins, n)\n",
        "\n",
        "# plt.plot(x, hp.Expo(x, *params), label = \"Expo fit\")\n",
        "# bkgest, bkgest_err =  hp.integral_with_errors(hp.Expo, x, (400,1100), params, errs)\n",
        "# print (f\"Our estimated background yield  is {bkgest:.2f} +/- {bkgest_err:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7a033e-46a0-4af6-9bb4-a911674be9ef",
      "metadata": {
        "id": "ad7a033e-46a0-4af6-9bb4-a911674be9ef"
      },
      "source": [
        "### Determining if you see an excess\n",
        "\n",
        "In the final session, we will repeat the background estimate procedure with the “observed” data: the parametrisation will be a little different but the estimate ought to be done in the same way. Then you will have two numbers: first, the estimated number of events in the SR from the sideband fit (your expected background), and the uncertainty on that number; second, the actual number of events you count in the SR. What you are trying to do is determine if you see an excess of observed events compared to the prediction. The prediction is effectively your null hypothesis (there is no X boson). You are trying to see if the data you observe are closer to the alternative hypothesis that the X boson exists. We define the significance\n",
        "\n",
        "$$Z = (N_\\text{predicted} − N_{\\text{observed}})/ \\text{err}_\\text{predicted},$$\n",
        "\n",
        "where $\\text{err}_\\text{predicted}$ is the uncertainty on your predicted number of events. If you see an “excess” of events, we can try to gauge how likely this was to have come from a statistical fluctuation using the significance.\n",
        "\n",
        "A significance of 1 (aka “1-σ”) is usually consistent with a statistical fluctuation.\n",
        "A 3-σ detection has a 0.3% probability of occurring by chance, and 5-σ is just a 0.00006% probability of occurring by chance. Physicists traditionally call a 3-σ detection “evidence”, while a 5-σ detection is considered a “discovery”...\n",
        "Let’s continue to practice with the simulated data to rehearse what might happen next week."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c9d71e1-28e9-46ee-ac10-f47652a0bbe7",
      "metadata": {
        "id": "6c9d71e1-28e9-46ee-ac10-f47652a0bbe7"
      },
      "source": [
        "**TASK 4.3:** Start by checking the SR using the dataset composed only of your background simulation. In other words, evaluate the sum of weights for events which are actually in the SR (not the ones from sideband estimate, but whose which really pass the selection and with `m_inv` $\\in [400, 1100]$ GeV)... and evaluate the significance.\n",
        "\n",
        "In principle, the observed and expected number of events should agree in this case, if your parametrisation was good! And therefore the significance should be low (since there is no X boson in this dataset, and no excess!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e95d68-a201-43c1-8246-873647e1c2f4",
      "metadata": {
        "id": "70e95d68-a201-43c1-8246-873647e1c2f4"
      },
      "outputs": [],
      "source": [
        "# Hint: you can amend the code below to work with your code\n",
        "# do you understand what each step is doing?\n",
        "\n",
        "# df_bkg_SR = df_bkg_selection[~mask_sideband_bkg]\n",
        "# n_obs = np.sum(df_bkg_SR.w)\n",
        "\n",
        "# Z = (n_obs - bkgest)/ bkgest_err\n",
        "\n",
        "# print (f\" Nobs = {n_obs:.2f}, Npred = {bkgest:.2f}+/-{bkgest_err:.2f}, Z = {Z:.2f}σ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccc7ca5e-1d22-4ec7-a8d6-182218f0b5be",
      "metadata": {
        "id": "ccc7ca5e-1d22-4ec7-a8d6-182218f0b5be"
      },
      "source": [
        "**TASK 4.4:** Repeat the exercise, but this time, counting the sum of eights of events in the SR with the background and one of your signal samples (pandas.concat is useful to add dataframes together). Do the expected and observed number\n",
        "of events agree this time? If not, what is the significance? If this happens with your \"real\" dataset, will you be able to claim a discovery ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531de694-fa8a-4663-9aaf-1fc53ed711db",
      "metadata": {
        "id": "531de694-fa8a-4663-9aaf-1fc53ed711db"
      },
      "outputs": [],
      "source": [
        "# Hint: you can amend the code below to work with your code\n",
        "# do you understand what each step is doing?\n",
        "\n",
        "# mask_SR_sig500 = np.logical_and(df_sig500_selection[\"minv\"]> 400, df_sig500_selection[\"minv\"]< 1100)\n",
        "# df_sig500_SR = df_sig500_selection[mask_SR_sig500]\n",
        "\n",
        "# df_sigBkg_SR = pd.concat([df_sig500_SR, df_bkg_SR])\n",
        "\n",
        "# n_obs = np.sum(df_sigBkg_SR.w)\n",
        "\n",
        "# Z = (n_obs - bkgest)/ bkgest_err\n",
        "\n",
        "# print (f\" Nobs = {n_obs:.2f}, Npred = {bkgest:.2f}+/-{bkgest_err:.2f}, Z = {Z:.2f}σ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072acae6-1344-4e6a-86f4-43b2ded2bb76",
      "metadata": {
        "id": "072acae6-1344-4e6a-86f4-43b2ded2bb76"
      },
      "source": [
        "## If you see an excess (3-σ or more)... and what to do if not."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d92e76-fa0a-407a-b073-4439ac60600e",
      "metadata": {
        "id": "70d92e76-fa0a-407a-b073-4439ac60600e"
      },
      "source": [
        "**If you see an excess** in your data which is statistically significant (> 3σ), you will need to quantify it in terms of its significance. People will want to know the mass of the boson you claim to have discovered. To do that you can plot the `m_inv` distribution. You should be able to see a bump on your spectrum at the mass of the discovered boson! Thee size of the bump will depend on the cross-section. One would determine those precisely with a fit.\n",
        "\n",
        "**If you do not see an excess** in your data, you will want to set a limit on the maximum allowed cross-section (if the cross-section was large, you'd have seen it, so it must be smaller than a certain value). This is done with the CLs method."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77e9d13-0e38-4f34-bf48-d2c248f2fe89",
      "metadata": {
        "id": "f77e9d13-0e38-4f34-bf48-d2c248f2fe89"
      },
      "source": [
        "# Part 5: Try it on your own dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecc7875-3529-4856-8094-48887be7533e",
      "metadata": {
        "id": "0ecc7875-3529-4856-8094-48887be7533e"
      },
      "source": [
        "Now you will be assigned a number, which corresponds to an unique dataset in which there may be hidden an X boson of a randomly-chosen mass and cross-section.\n",
        "\n",
        "Your job is not to:\n",
        "- read your dataset into a dataframe\n",
        "- apply your selection\n",
        "- plot the invariant mass spectrum\n",
        "- do a fit of the sidebands and determine the background estimate in the SR (and its uncertainty).\n",
        "- compare it to the observed number of events you observed and determine the significance.\n",
        "\n",
        "NB: the data does not need any normalisation weights! (ie all weights=1)\n",
        "\n",
        "Before we start, let's check if you have all the experimental datasets available:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls SearchingTheXBoson/datasets_experiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE6dG0Ylw04-",
        "outputId": "2fea02ae-ab2f-41c2-b3ee-a836daccb3da"
      },
      "id": "oE6dG0Ylw04-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_0.csv  data_2.csv\tdata_4.csv  data_6.csv\tdata_8.csv\n",
            "data_1.csv  data_3.csv\tdata_5.csv  data_7.csv\tdata_9.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's generate a random number for you! Replace the seed below with your favourite number and run the cell."
      ],
      "metadata": {
        "id": "Vl3dR5C5r1LC"
      },
      "id": "Vl3dR5C5r1LC"
    },
    {
      "cell_type": "code",
      "source": [
        "seed = ???\n",
        "\n",
        "import numpy as np\n",
        "rng = np.random.default_rng(seed)\n",
        "number = rng.integers(0,10)\n",
        "\n",
        "print(f\"You got dataset data_{number}.csv\")"
      ],
      "metadata": {
        "id": "oG-PE4Ijr21z"
      },
      "id": "oG-PE4Ijr21z",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5.1:** Perform the data analysis in your dataset and get the significance. Did you make a discovery or refute Dr. X's theory ?"
      ],
      "metadata": {
        "id": "I_BcL6Y2wzvR"
      },
      "id": "I_BcL6Y2wzvR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60215f9-19bd-4c62-8997-3276060da245",
      "metadata": {
        "id": "a60215f9-19bd-4c62-8997-3276060da245"
      },
      "outputs": [],
      "source": [
        "## Determine your signifiance below!\n",
        "\n",
        "# open the dataset and add invariant mass variable\n",
        "\n",
        "# apply the selection\n",
        "\n",
        "# define the sideband and SR\n",
        "\n",
        "# do the fit like before, maybe something along these lines?\n",
        "\n",
        "# n, bins, _ = plt.hist(df_data_sideband.minv, bins=np.linspace(50,2000, 40))\n",
        "# plt.hist(df_data_SR.minv, bins=np.linspace(50,2000, 40))\n",
        "# x, params, errs = hp.do_fit(hp.Expo, bins, n)\n",
        "# hp.fixedExpoN, hp.fixedExpoL = params[0], params[1]\n",
        "# plt.plot(x, hp.Expo(x, *params), label = \"Expo fit\")\n",
        "# bkgest, bkgest_err =  hp.integral_with_errors(hp.Expo, x, (400,1100), params, errs)\n",
        "# print (f\"Our estimated background yield  is {bkgest:.2f} +/- {bkgest_err:.2f}\")\n",
        "\n",
        "# get the observed yield by counting data events\n",
        "\n",
        "# n_obs = len(df_data_SR)\n",
        "# Z = (n_obs - bkgest)/ bkgest_err\n",
        "# print (f\"Nobs = {n_obs:.2f}, Npred = {bkgest:.2f}+/-{bkgest_err:.2f}, Z = {Z:.2f}σ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2e13d9-74ab-4a21-addc-bc9c094113ee",
      "metadata": {
        "id": "ad2e13d9-74ab-4a21-addc-bc9c094113ee"
      },
      "source": [
        "(If you made a discovery) **Task 5.2a:** What is the mass of the discovered boson, and it's cross-section?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496741b9-aa74-4c48-a69b-d7097c82994d",
      "metadata": {
        "id": "496741b9-aa74-4c48-a69b-d7097c82994d"
      },
      "outputs": [],
      "source": [
        "## hint: uncomment the code below and amend it to work in your code\n",
        "\n",
        "# n, bins, _ = plt.hist(df_data_selection.minv, bins=np.linspace(50,2000, 40))\n",
        "\n",
        "# x, params, errs = hp.do_fit(hp.GausPlusFixedExpo, bins, n)\n",
        "# plt.plot(x, hp.GausPlusFixedExpo(x, *params), label = \"Expo plus Gaussian fit\")\n",
        "# y, ye = hp.integral_with_errors(hp.Gaussian, x, (400,1100), params[:3], errs[:3])\n",
        "\n",
        "# xsFactor = 5.4/sum(df_sig500_selection.w) # this depends on your own cuts\n",
        "# print(params, hp.fixedExpoN, hp.fixedExpoL)\n",
        "# print(\"measured yield: %.2f +/- %.2f \" % ( y, ye))\n",
        "# print(\"measured cross-section: %.2f +/- %.2f pb\" % (  y*xsFactor, ye*xsFactor))\n",
        "# print(\"measured mass: %.2f +/- %.2f GeV\" % ( params[0], errs[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e604ae-5cb3-4a0a-86b2-fedf16b0e17c",
      "metadata": {
        "id": "24e604ae-5cb3-4a0a-86b2-fedf16b0e17c"
      },
      "source": [
        "(If you saw no excess) **Task 5.2b:** What are the limits according to the CLs method?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86e9e7f-4382-418c-99f2-e1003b46fe79",
      "metadata": {
        "id": "d86e9e7f-4382-418c-99f2-e1003b46fe79"
      },
      "outputs": [],
      "source": [
        "## hint: uncomment the code below to determine the excluded region\n",
        "\n",
        "# signal_pred = sum(df_sig500_selection.w)\n",
        "# x = hp.cls_limit_calculator([signal_pred], [bkgest], [bkgest_err], [n_obs])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}